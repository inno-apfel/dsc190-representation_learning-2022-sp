{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST data\n",
    "\n",
    "Now let's look at a slightly larger and more interesting dataset: the MNIST handwritten image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [[ ! -e \"mnist.npz\" ]]; then\n",
    "    wget 'https://f000.backblazeb2.com/file/jeldridge-data/mnist.npz'\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thin_by = 3\n",
    "mnist_data = np.load('mnist.npz')\n",
    "mnist_train_features = mnist_data['train'].T.astype(float)[::thin_by]\n",
    "mnist_train_labels = mnist_data['train_labels'].flatten()[::thin_by]\n",
    "mnist_test_features = mnist_data['test'].T.astype(float)[::thin_by]\n",
    "mnist_test_labels = mnist_data['test_labels'].flatten()[::thin_by]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now in a $20,000 \\times 784$ array. There are 20,000 examples, each being a 784-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these vectors is actually a 28x28 image, \"flattened\" into a vector. We can reshape and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mnist_train_features[10_000].reshape(28, -1), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigendigits\n",
    "\n",
    "Images typically contain many correlated features, since nearby pixels behave similar to one another. We can apply PCA to find a basis in which the coordinates are decorrelated. First, we center the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_train_features.T\n",
    "mu = X.mean(axis=1)\n",
    "X = X - mu[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = X @ X.T / X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we find the eigenvalues and eigenvectors of $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals, eigvecs = np.linalg.eigh(C)\n",
    "\n",
    "# reorder eigenvalues/vectors from largest to smallest\n",
    "eigvals = eigvals[::-1]\n",
    "eigvecs = eigvecs[:,::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image in the data set is a vector in $\\mathbb R^{784}$, and each eigenvector of $C$ is a vector in the same space. As a result, we can visualize the eigenvectors in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.matshow(eigvecs[:,i].reshape(-1, 28))\n",
    "    plt.title(f'Eigenvector {i+1}')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eigvals)\n",
    "plt.title(\"Elbow Plot of Eigenvalues of C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any $28 \\times 28$ image can be written as a linear combination of these eigenvectors. For instance, consider the image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((mu + X[:,10_000]).reshape(-1, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = eigvecs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for K in [10, 20, 40, 60, 80, 160, 320, 640, 784]:\n",
    "    P = Q[:K]\n",
    "    z = P.T @ P @ X[:, 10_000] + mu\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(f'K = {K}')\n",
    "    plt.imshow(z.reshape(-1, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But really, *any* $28 \\times 28$ image can be decomposed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('mystery.jpeg').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for K in [10, 20, 40, 60, 80, 160, 320, 640, 784]:\n",
    "    P = Q[:K]\n",
    "    z = P.T @ P @ (img.flatten() - mu) + mu\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(f'K = {K}')\n",
    "    plt.imshow(z.reshape(-1, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The kNN Classifier\n",
    "\n",
    "First, we will test a kNN classifier on the original data set with no added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors\n",
    "\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(mnist_train_features, mnist_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.random.choice(len(mnist_test_features), 200)\n",
    "knn.score(mnist_test_features[ix], mnist_test_labels[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the classifier achieves about a 95% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Noise\n",
    "\n",
    "Now we will add many noisy dimensions to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_NEW_ROWS = 28*3\n",
    "NOISE_MU = 200\n",
    "NOISE_SIGMA = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noisy_dimensions(data):\n",
    "    noisy_data = np.pad(data, [[0, 0], [0, NUMBER_OF_NEW_ROWS * 28]], 'constant')\n",
    "    appended_shape = (noisy_data.shape[0], NUMBER_OF_NEW_ROWS*28)\n",
    "    noisy_data += np.random.normal(NOISE_MU, NOISE_SIGMA, noisy_data.shape)\n",
    "    return np.clip(noisy_data, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_train_features = add_noisy_dimensions(mnist_train_features)\n",
    "noisy_test_features = add_noisy_dimensions(mnist_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noisy_train_features[10_000].reshape(-1, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the kNN classification performance suffer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(noisy_train_features, mnist_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.random.choice(len(mnist_test_features), 200)\n",
    "knn.score(noisy_test_features[ix], mnist_test_labels[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new noisy dimensions have \"confused\" the classifier, and its accuracy is diminished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with PCA\n",
    "\n",
    "Let's reduce the dimensionality with PCA. First, we center the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = noisy_train_features.T\n",
    "mu = X_train.mean(axis=1)\n",
    "X_train = X_train - mu[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = X_train @ X_train.T / X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the eigenvalues and eigenvectors of $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals, eigvecs = np.linalg.eigh(C)\n",
    "\n",
    "# reorder so that largest eigenvalues are first\n",
    "eigvals = eigvals[::-1]\n",
    "eigvecs = eigvecs[:,::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the change of basis matrix, $Q$, whose rows are the eigenvectors of $C$. The coordinates of $\\vec x$ in the new basis are given by $Q \\vec x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = eigvecs.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep only $K$ eigenvectors with the greatest eigenvalues. But how do we choose $K$? One approach is the \"elbow\" method. Intuitively, the top eigenvectors capture useful variation while the bottom eigenvectors capture not-as-useful information (noise). The split between the \"top\" and \"bottom\" eigenvectors is usually not clearly defined, but we can make a good choice by plotting the eigenvalues and finding the point where they level out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eigvals[:50])\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Eigenvalue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the top 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = Q[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train = P @ X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification performance after dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = noisy_test_features.T - mu[:,None]\n",
    "Z_test = P @ X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(Z_train.T, mnist_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.random.choice(len(mnist_test_features), 1000)\n",
    "knn.score(Z_test.T[ix], mnist_test_labels[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are close to where we were before the noise was added!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
